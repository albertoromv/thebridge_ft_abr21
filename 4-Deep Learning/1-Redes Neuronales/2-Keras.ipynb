{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPor defecto, keras tira de GPU\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install keras\n",
    "'''\n",
    "Por defecto, keras tira de GPU\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train --> 60 000 fotos, cada una tiene 28x28 pixels   y_train --> 60 000 etiquetas\\n\\nfoto_0 -------> 0\\nfoto_1 -------> 0\\nfoto_2 -------> 0\\nfoto_3 -------> 1\\nfoto_4 -------> 1\\nfoto_5 -------> 3\\n\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_train --> 60 000 fotos, cada una tiene 28x28 pixels   y_train --> 60 000 etiquetas\n",
    "\n",
    "foto_0 -------> 0\n",
    "foto_1 -------> 0\n",
    "foto_2 -------> 0\n",
    "foto_3 -------> 1\n",
    "foto_4 -------> 1\n",
    "foto_5 -------> 3\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d887fc2e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap(\"Greys\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays). Aplano a una dimension cada imagen.\n",
    "# Escalamos ya que vamos a usar gradient descent, y le afecta mucho la escala de las features.\n",
    "# Ejecutar esta celda solo una vez. Si no, reescalará\n",
    "\n",
    "# 0 - 255 --> 0 - 1\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10,000 samples for validation. Entraran dentro del modelo para validar. No es validacion cruzada\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una manera de declarar la red neuronal\n",
    "\n",
    "# Siempre hay que declarar la capa sequential para empezar a declarar la red\n",
    "# Se trata de la API sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Flatten, aplana en un unico vector. Y especificamos el tamaño de la entrada\n",
    "# Es como si hiciese un .reshape(-1, 28*28)\n",
    "# \"kernel_initializer\" o \"bias_initializer\" No lo usamos pero seria para inicializar los pesos de otra manera\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Capas de la red. Dense es la capa de neuronas. Necesitamos numero y activacion\n",
    "model.add(keras.layers.Dense(units = 300, activation = \"relu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Capa de salida, con tamaño del número de clases\n",
    "# Suele ir aqui un softmax. Para multiclase guay. Si es binaria -> sigmoide\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.dense.Dense object at 0x000002D8867036A0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x2d8867039a0>,\n",
       " <keras.layers.core.dense.Dense at 0x2d8867036a0>,\n",
       " <keras.layers.core.dense.Dense at 0x2d886703670>,\n",
       " <keras.layers.core.dense.Dense at 0x2d886703760>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[1])\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "# 784 features (pixeles de las imagenes) x 300 neuronas\n",
    "# Los pesos están inicializados aleatoriamente\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica la configuración del entrenamiento (optimizador, pérdida, métricas):\n",
    "model.compile(\n",
    "    # Stocastic gradient descent. El algoritmo para minimizar la loss function\n",
    "    # El stocastic va haciendo muestreo en cada evaluacion, no usa todo.\n",
    "    # Podemos modificar el learning rate(0.01 por defecto) mediante el parametro lr\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    \n",
    "    # Loss function to minimize\n",
    "    # sparse_categorical_crossentropy cuando tenemos un label en nuna columna\n",
    "    # Si lo tuviesemos en varias tipo dummy, cogeriamos categorical_crossentropy\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(optimizer=\"sgd\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "# La primera hidden layer tiene 784 entradas x 300 salidas\n",
    "# Son los 235500 params = 783x300 + 300 (bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit model on training data\n",
      "Epoch 1/15\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.9305 - accuracy: 0.7671 - val_loss: 0.4044 - val_accuracy: 0.8969\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3809 - accuracy: 0.8948 - val_loss: 0.3068 - val_accuracy: 0.9132\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3127 - accuracy: 0.9115 - val_loss: 0.2676 - val_accuracy: 0.9251\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2771 - accuracy: 0.9210 - val_loss: 0.2438 - val_accuracy: 0.9318\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2520 - accuracy: 0.9288 - val_loss: 0.2253 - val_accuracy: 0.9353\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.2323 - accuracy: 0.9346 - val_loss: 0.2080 - val_accuracy: 0.9403\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.2156 - accuracy: 0.9387 - val_loss: 0.1962 - val_accuracy: 0.9439\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.2013 - accuracy: 0.9427 - val_loss: 0.1879 - val_accuracy: 0.9465\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1884 - accuracy: 0.9464 - val_loss: 0.1750 - val_accuracy: 0.9510\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.1770 - accuracy: 0.9492 - val_loss: 0.1687 - val_accuracy: 0.9520\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1669 - accuracy: 0.9518 - val_loss: 0.1634 - val_accuracy: 0.9542\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.1575 - accuracy: 0.9543 - val_loss: 0.1529 - val_accuracy: 0.9567\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.1492 - accuracy: 0.9569 - val_loss: 0.1470 - val_accuracy: 0.9581\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.1420 - accuracy: 0.9592 - val_loss: 0.1420 - val_accuracy: 0.9596\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1349 - accuracy: 0.9612 - val_loss: 0.1355 - val_accuracy: 0.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEn el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\\nSi vemos que ya no baja mas, no serán necesarias tantas epochs.\\nImprimera tantas lineas como epochs hayamos puesto\\n\\nTampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\\nUtil para datasets desbalanceados.\\n\\nEl loss que muestra es el categoricalcrossentropy\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\n",
    "print(\"fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 15,\n",
    "validation_data = (X_val, y_val)\n",
    ")\n",
    "\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    # En vez de validation data podemos usar el argumento validation_split=0.1\n",
    "    \n",
    "\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781.25"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)/64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit model on training data\n",
      "Epoch 1/15\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1287 - accuracy: 0.9630 - val_loss: 0.1321 - val_accuracy: 0.9626\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.1230 - accuracy: 0.9650 - val_loss: 0.1278 - val_accuracy: 0.9644\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1174 - accuracy: 0.9671 - val_loss: 0.1259 - val_accuracy: 0.9644\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1121 - accuracy: 0.9682 - val_loss: 0.1244 - val_accuracy: 0.9656\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1074 - accuracy: 0.9697 - val_loss: 0.1205 - val_accuracy: 0.9662\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1031 - accuracy: 0.9711 - val_loss: 0.1191 - val_accuracy: 0.9662\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0990 - accuracy: 0.9721 - val_loss: 0.1124 - val_accuracy: 0.9687\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0949 - accuracy: 0.9737 - val_loss: 0.1105 - val_accuracy: 0.9682\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.0913 - accuracy: 0.9746 - val_loss: 0.1081 - val_accuracy: 0.9696\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.0881 - accuracy: 0.9754 - val_loss: 0.1063 - val_accuracy: 0.9703\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.0848 - accuracy: 0.9767 - val_loss: 0.1066 - val_accuracy: 0.9704\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.0816 - accuracy: 0.9775 - val_loss: 0.1032 - val_accuracy: 0.9704\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0788 - accuracy: 0.9787 - val_loss: 0.1019 - val_accuracy: 0.9712\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0758 - accuracy: 0.9796 - val_loss: 0.0983 - val_accuracy: 0.9729\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 0.0733 - accuracy: 0.9800 - val_loss: 0.0982 - val_accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "print(\"fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 15,\n",
    "validation_data = (X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 15, 'steps': 782}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.12867343425750732,\n",
       "  0.12296117842197418,\n",
       "  0.11735933274030685,\n",
       "  0.11208629608154297,\n",
       "  0.10743284970521927,\n",
       "  0.10305939614772797,\n",
       "  0.09899760782718658,\n",
       "  0.09489226341247559,\n",
       "  0.0912662073969841,\n",
       "  0.08805843442678452,\n",
       "  0.08482921868562698,\n",
       "  0.08162510395050049,\n",
       "  0.07876212894916534,\n",
       "  0.07584592700004578,\n",
       "  0.07329331338405609],\n",
       " 'accuracy': [0.9629600048065186,\n",
       "  0.9649800062179565,\n",
       "  0.9670799970626831,\n",
       "  0.9681800007820129,\n",
       "  0.9696999788284302,\n",
       "  0.9710999727249146,\n",
       "  0.9721199870109558,\n",
       "  0.9737399816513062,\n",
       "  0.974560022354126,\n",
       "  0.975380003452301,\n",
       "  0.9766799807548523,\n",
       "  0.9774799942970276,\n",
       "  0.9786999821662903,\n",
       "  0.9796000123023987,\n",
       "  0.9800400137901306],\n",
       " 'val_loss': [0.1321360021829605,\n",
       "  0.1278219074010849,\n",
       "  0.1258515864610672,\n",
       "  0.12440808117389679,\n",
       "  0.12051474303007126,\n",
       "  0.11907895654439926,\n",
       "  0.11241123080253601,\n",
       "  0.11047441512346268,\n",
       "  0.10808586329221725,\n",
       "  0.10632321983575821,\n",
       "  0.10661128908395767,\n",
       "  0.10319656133651733,\n",
       "  0.10190863162279129,\n",
       "  0.0982859805226326,\n",
       "  0.09819166362285614],\n",
       " 'val_accuracy': [0.9625999927520752,\n",
       "  0.9643999934196472,\n",
       "  0.9643999934196472,\n",
       "  0.9656000137329102,\n",
       "  0.9661999940872192,\n",
       "  0.9661999940872192,\n",
       "  0.9686999917030334,\n",
       "  0.9682000279426575,\n",
       "  0.9696000218391418,\n",
       "  0.970300018787384,\n",
       "  0.9703999757766724,\n",
       "  0.9703999757766724,\n",
       "  0.9711999893188477,\n",
       "  0.9728999733924866,\n",
       "  0.9718000292778015]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Podemos ver como evoluciona el entrenamiento, en funcion de los epochs\n",
    "# Validacion y training estan muy cerca, no hay overfitting!\n",
    "# Todavia no ha acabado de coverger ya que el loss en validacion sigue bajando,\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0966 - accuracy: 0.9700\n",
      "test loss, test acc: [0.09656545519828796, 0.9700000286102295]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el \"score\" a partir del conjunto de test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "\n",
    "# Metodo evaluate para que nos de el error vs las metricas elegidas en la funcion compile\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap = plt.cm.get_cmap(\"Greys\")); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 537ms/step\n",
      "predictions shape: (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Ojo aqui viene slicing xq presupone que le entran varios inputs\n",
    "Nos da las probabilidades de pertenecer a una clase u otra.\n",
    "'''\n",
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing[\"target\"]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                             housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 0.8057 - val_loss: 0.5320\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.3196 - val_loss: 0.8629\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5032 - val_loss: 0.4756\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4561 - val_loss: 0.4490\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4433 - val_loss: 0.4403\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4301 - val_loss: 0.4281\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4194 - val_loss: 0.4385\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4145 - val_loss: 0.4154\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.4126\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4006 - val_loss: 0.4059\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3973 - val_loss: 0.4108\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3901 - val_loss: 0.3962\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3846 - val_loss: 0.3920\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3799 - val_loss: 0.3901\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3757 - val_loss: 0.3785\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3733 - val_loss: 0.3780\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3707 - val_loss: 0.3873\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3676 - val_loss: 0.3686\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3618 - val_loss: 0.3702\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3588 - val_loss: 0.3667\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.3770\n",
      "1/1 [==============================] - 0s 199ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # no hace falta capa de flatten, no hay que aplanar nada\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\", optimizer =\"sgd\")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data = (X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # como si fueran instancias nuevas\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Sirven para que el modelo se vaya guardando tras cada epoch, asi no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.3579\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3545\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3538\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3494\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3476\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3495\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3462\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 0.3445\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 0.3403\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3399\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 4s 12ms/step - loss: 0.3430 - val_loss: 0.3434\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3378 - val_loss: 0.3445\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 4s 11ms/step - loss: 0.3370 - val_loss: 0.3444\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 4s 10ms/step - loss: 0.3335 - val_loss: 0.3373\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 4s 10ms/step - loss: 0.3395 - val_loss: 0.5968\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 4s 10ms/step - loss: 0.4249 - val_loss: 0.3503\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3761 - val_loss: 0.3475\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 4s 12ms/step - loss: 0.3412 - val_loss: 0.3380\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3382 - val_loss: 0.4513\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3392 - val_loss: 0.3392\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3322 - val_loss: 0.3358\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3334 - val_loss: 0.3319\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3311 - val_loss: 0.3332\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3301 - val_loss: 0.3358\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3295 - val_loss: 0.3307\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3259 - val_loss: 0.3293\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3381 - val_loss: 0.3361\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3307 - val_loss: 0.3301\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3279 - val_loss: 0.3381\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3261 - val_loss: 0.3299\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs = 10, callbacks = [checkpoint_cb])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3276 - val_loss: 0.3520\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3394 - val_loss: 0.3691\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3236 - val_loss: 0.4377\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3317 - val_loss: 0.3622\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3311 - val_loss: 0.3538\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3251 - val_loss: 0.3541\n"
     ]
    }
   ],
   "source": [
    "# 10 esta bien. Lo pondemos a 5 para el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Keras tiene implementado un dashboard para monitorizar las ejecuciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3267 - val_loss: 0.3540\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3260 - val_loss: 0.3719\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3248 - val_loss: 0.3584\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3305 - val_loss: 0.3463\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3224 - val_loss: 0.3525\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3216 - val_loss: 0.3499\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3209 - val_loss: 0.3574\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3234 - val_loss: 0.3808\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3234 - val_loss: 0.3478\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3219 - val_loss: 0.8798\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3307 - val_loss: 0.3631\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3182 - val_loss: 0.3607\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3189 - val_loss: 0.3450\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3167 - val_loss: 0.3470\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3230 - val_loss: 0.4674\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3322 - val_loss: 0.3491\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3202 - val_loss: 0.3462\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3155 - val_loss: 0.3491\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3182 - val_loss: 0.3556\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3166 - val_loss: 0.3470\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3174 - val_loss: 0.3424\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3162 - val_loss: 0.4015\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3143 - val_loss: 0.3512\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3156 - val_loss: 0.3600\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3199 - val_loss: 1.2894\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3539 - val_loss: 0.3634\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3133 - val_loss: 0.3595\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3156 - val_loss: 0.3872\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3133 - val_loss: 0.3480\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3151 - val_loss: 0.3509\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3338 - val_loss: 0.3492\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3125 - val_loss: 0.3476\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.3715\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3195 - val_loss: 0.3515\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3170 - val_loss: 0.4182\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3152 - val_loss: 0.3779\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3176 - val_loss: 0.3421\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3130 - val_loss: 0.3486\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3124 - val_loss: 0.4086\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3166 - val_loss: 0.3559\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3165 - val_loss: 0.3477\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3202 - val_loss: 0.3585\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3102 - val_loss: 0.3396\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3134 - val_loss: 0.3484\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3120 - val_loss: 0.3595\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3141 - val_loss: 0.3360\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3088 - val_loss: 0.3429\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.3335\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3135 - val_loss: 0.3612\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.3453\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPara lanzarlo desde el jupyter notebook\\n%load_ext tensorboard\\n%tensorboard --logdir=./my_logs --port=6006\\n\\nPara lanzarlo desde el terminal, hay que estar en la carpeta de los logs\\ntensorboard --logdir=./my_logs --port=6006\\n\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19596), started 3 days, 11:47:51 ago. (Use '!kill 19596' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f5ad04d628d188c9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f5ad04d628d188c9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
